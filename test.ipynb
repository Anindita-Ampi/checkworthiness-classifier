{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbde29a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (/Users/aninditakundu/pytorch-test/env/lib/python3.8/site-packages/keras/preprocessing/sequence.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msentiment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentimentIntensityAnalyzer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding, Input, Dense, concatenate\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (/Users/aninditakundu/pytorch-test/env/lib/python3.8/site-packages/keras/preprocessing/sequence.py)"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Input, Dense, concatenate\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427bd7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained word embeddings\n",
    "embeddings_index = {}\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('sentiment_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f65b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculate sentiment score and sentence length for each sentence\n",
    "sentiment_scores = []\n",
    "sentence_lengths = []\n",
    "for sentence in data['text']:\n",
    "    sentiment_scores.append(sid.polarity_scores(sentence)['compound'])\n",
    "    sentence_lengths.append(len(sentence.split()))\n",
    "\n",
    "# Normalize sentiment score and sentence length\n",
    "sentiment_scores = (np.array(sentiment_scores) - np.mean(sentiment_scores)) / np.std(sentiment_scores)\n",
    "sentence_lengths = (np.array(sentence_lengths) - np.mean(sentence_lengths)) / np.std(sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2477c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['text'])\n",
    "sequences = tokenizer.texts_to_sequences(data['text'])\n",
    "word_index = tokenizer.word_index\n",
    "max_sequence_length = max(len(s) for s in sequences)\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "data = pad_sequences(sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a4a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b086bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input layers for word embeddings, sentiment score, and sentence length\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "sentiment_input = Input(shape=(1,))\n",
    "sentence_length_input = Input(shape=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5182f626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply embedding layer to input data\n",
    "embedded_sequences = embedding_layer(data)\n",
    "\n",
    "# Flatten embedding output\n",
    "embedded_sequences = Flatten()(embedded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c057007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate word embeddings, sentiment score, and sentence length features\n",
    "x = concatenate([embedded_sequences, sentiment_input, sentence_length_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea4caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fully connected layers\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "output = Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3cce8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736b768e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec6e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = Model(inputs=[embedding_layer.input, sentiment_input, sentence_length_input], outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit([data, sentiment_scores, sentence_lengths], data['label'], validation_split=0.2, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate([data, sentiment_scores, sentence_lengths], data['label'], verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ffcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Load pre-trained word embeddings\n",
    "embeddings_index = {}\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('text_data.csv')\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['text'])\n",
    "sequences = tokenizer.texts_to_sequences(data['text'])\n",
    "word_index = tokenizer.word_index\n",
    "max_sequence_length = max(len(s) for s in sequences)\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Create embedding matrix\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Initialize embedding layer\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "\n",
    "# Define model architecture\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(data, data['label'], validation_split=0.2, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(data, data['label'], verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
